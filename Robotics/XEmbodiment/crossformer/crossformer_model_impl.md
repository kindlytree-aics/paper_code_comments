# å…³äºCrossFormer Modelå®ç°çš„å‡ ä¸ªé—®é¢˜åŠè§£ç­”

## é—®é¢˜1ï¼šè¯¥å¼€æºä»£ç å®ç°ä¸­è¯­è¨€æŒ‡ä»¤ï¼ˆlanguage instructionsï¼‰å’Œå›¾åƒèåˆçš„FiLMä»£ç å…·ä½“çš„ä½“ç°åœ¨ä»€ä¹ˆåœ°æ–¹ï¼Ÿ

å›ç­”ï¼šå…¶ä¸­åœ¨å‡½æ•°get_model_config(./scripts/configs/pretrain_config.py)é‡Œï¼Œå®šä¹‰äº†ResNet26FILMç±»ä¸ºå›¾åƒç±»çš„ç¼–ç å™¨ã€‚
è¯¥ç±»æˆå‘˜å˜é‡use_filmè®¾ä¸ºTrueï¼Œå› æ­¤é‡‡ç”¨äº†film_conditioning_layerå®ç°å°†æ¡ä»¶å˜é‡ï¼ˆä¸€èˆ¬ä¸ºlanguage instructionsç”Ÿæˆçš„è¯­ä¹‰å¼ é‡ä¿¡æ¯ï¼Œ
å¦‚é€šè¿‡BERTè¾“å‡ºçš„å¯¹åº”[CLS]ä½ç½®çš„embeddingså¼ é‡ï¼Œåœ¨ä»£ç å®ç°ä¸­ä¸ºUniversalSentenceEncoderï¼Œå®ç°ä½ç½®åœ¨./data/utils/text_processing.pyä¸­ï¼Œ
åœ¨pretrain_config.pyçš„å®šä¹‰æ–¹å¼ä¸º:ModuleSpec.create(UniversalSentenceEncoder)ï¼‰

[åˆ†è¯å™¨å®ç°](https://github.com/rail-berkeley/crossformer/blob/4a56b64411e7ec039ea6cce6bbbe65a38f677db1/crossformer/model/crossformer_module.py#L159)ã€‚æ ¹æ®é€»è¾‘å’ŒRT-1ä¸­ç±»ä¼¼çš„å®ç°ï¼Œè¯¥èåˆåœ¨Transformerè¿›è¡Œæ³¨æ„åŠ›æœºåˆ¶è®¡ç®—ä¹‹å‰ï¼Œåœ¨Transformeræ¨¡å‹è¾“å…¥æ•°æ®æ—¶å·²ç»åšå¥½ç›¸å…³çš„èåˆå·¥ä½œï¼Œå…·ä½“è§£é‡Šå¦‚ä¸‹ï¼š  
æ•´ä¸ªæ¨¡å‹ç±»ä¸ºCrossFormerTransformerï¼Œæ¨¡å‹è¾“å…¥å¤„ç†çš„å…¥å£ä¸ºï¼šhttps://github.com/rail-berkeley/crossformer/blob/4a56b64411e7ec039ea6cce6bbbe65a38f677db1/crossformer/model/crossformer_module.py#L89

- å…³äºtaskåˆ†è¯å™¨çš„å®ç°
```
# å…³äºä»»åŠ¡åˆ†è¯å™¨çš„å®ç°
# task_tokenizers=dict(),ä½†æ˜¯åœ¨.\scripts\configs\pretrain_config.pyå®šä¹‰ä¸ºç©ºå­—å…¸ï¼Œå› æ­¤taskä¸ä½œä¸ºä¸“é—¨çš„tokensåºåˆ—

for name, tok in self.task_tokenizers.items():
    group_name = f"task_{name}"
    # Receive inputs from tokenizer and cast to embedding size
    tokenizer_output: TokenGroup = tok(observations, tasks, train=train)
    if tokenizer_output is None:
        logging.warning(f"Skipping task tokenizer: {group_name}")
        continue

    task_tokens = nn.Dense(
        self.token_embedding_size, name=f"{group_name}_projection"
    )(tokenizer_output.tokens)
    # task_tokens shape is (batch, n_tokens, token_embedding_size)

    # create positional embedding
    task_pos_enc = self._create_positional_embedding(group_name, task_tokens)

    all_prefix_groups.append(
        PrefixGroup(
            tokens=task_tokens,
            pos_enc=task_pos_enc,
            mask=tokenizer_output.mask,
            name=group_name,
            attention_rules=task_attention_rules,
        )
    )
```

- å…³äºobservationsåˆ†è¯å™¨çš„å®ç°
  
```
# https://github.com/rail-berkeley/crossformer/blob/4a56b64411e7ec039ea6cce6bbbe65a38f677db1/crossformer/model/crossformer_module.py#L189

for name, tok in self.observation_tokenizers.items():
    group_name = f"obs_{name}"
    # Receive inputs from tokenizer and cast to embedding size
    tokenizer_output: TokenGroup = tok(observations, tasks, train=train)
    if tokenizer_output is None:
        logging.warning(f"Skipping observation tokenizer: {group_name}")
        continue

    obs_tokens = nn.Dense(
        self.token_embedding_size, name=f"{group_name}_projection"
    )(tokenizer_output.tokens)
    # obs_tokens shape is (batch, horizon, n_tokens, token_embedding_size)

    # create positional embedding
    obs_pos_enc = self._create_positional_embedding(group_name, obs_tokens)

    # Update mask to account for which timesteps are padding
    obs_pad_mask = jnp.logical_and(
        timestep_pad_mask[:, :, None], tokenizer_output.mask
    )

    all_timestep_groups.append(
        TimestepGroup(
            tokens=obs_tokens,
            pos_enc=obs_pos_enc,
            mask=obs_pad_mask,
            name=group_name,
            attention_rules=observation_attention_rules,
        )
    )


# https://github.com/rail-berkeley/crossformer/blob/4a56b64411e7ec039ea6cce6bbbe65a38f677db1/scripts/configs/pretrain_config.py#L222

observation_tokenizers=dict(
    primary=ModuleSpec.create(
        ImageTokenizer,
        obs_stack_keys=["image_primary"],
        task_stack_keys=["image_primary"],
        task_film_keys=["language_instruction"],
        encoder=encoder,
    ),
    high=ModuleSpec.create(
        ImageTokenizer,
        obs_stack_keys=["image_high"],
        task_stack_keys=["image_high"],
        task_film_keys=["language_instruction"],
        encoder=encoder,
    ),
    nav=ModuleSpec.create(
        ImageTokenizer,
        obs_stack_keys=["image_nav"],
        task_stack_keys=["image_nav"],
        task_film_keys=[],
        encoder=ModuleSpec.create(ResNet26),
    ),
    left=ModuleSpec.create(
        ImageTokenizer,
        obs_stack_keys=["image_left_wrist"],
        task_stack_keys=[],
        task_film_keys=["language_instruction"],
        encoder=encoder,
    ),
    right=ModuleSpec.create(
        ImageTokenizer,
        obs_stack_keys=["image_right_wrist"],
        task_stack_keys=[],
        task_film_keys=["language_instruction"],
        encoder=encoder,
    ),
    bimanual=ModuleSpec.create(
        LowdimObsTokenizer,
        obs_keys=["proprio_bimanual"],
        dropout_rate=0.2,
    ),
    quadruped=ModuleSpec.create(
        LowdimObsTokenizer,
        obs_keys=["proprio_quadruped"],
    ),
),

# https://github.com/rail-berkeley/crossformer/blob/4a56b64411e7ec039ea6cce6bbbe65a38f677db1/crossformer/model/components/tokenizers.py#L143
# æ³¨æ„åœ¨æ•°æ®å‡†å¤‡é˜¶æ®µçš„process_batchå‡½æ•°é‡Œï¼Œå¯¹åŸºäºè‡ªç„¶è¯­è¨€çš„taskè¿›è¡Œç¼–ç ï¼Œå…·ä½“è°ƒç”¨çš„process_textå‡½æ•°
encoder_input_kwargs = {}
if self.task_film_keys:
    film_inputs = extract_inputs(self.task_film_keys, tasks)
    film_inputs = film_inputs[:, None].repeat(t, axis=1) #æ‰©å±•tä¸ªæ—¶é—´ç‚¹
    encoder_input_kwargs.update(
        {"cond_var": jnp.reshape(film_inputs, (b * t, -1))}
    )

# run visual encoder
encoder_def = ModuleSpec.instantiate(self.encoder)()
image_tokens = encoder_def(enc_inputs, **encoder_input_kwargs)
image_tokens = jnp.reshape(image_tokens, (b, t, -1, image_tokens.shape[-1]))

# https://github.com/rail-berkeley/crossformer/blob/4a56b64411e7ec039ea6cce6bbbe65a38f677db1/crossformer/model/components/vit_encoders.py#L261

else:
    if self.use_film:
        assert cond_var is not None, "Cond var is None, nothing to condition on"
        x = FilmConditioning()(x, cond_var)
```

## é—®é¢˜2ï¼šä¸åŒçš„è§‚æµ‹tokensç±»å‹ï¼ˆå¦‚å›¾åƒå’Œæœ¬ä½“è§‚æµ‹æ•°æ®ï¼‰å¦‚ä½•å®ç°çš„èåˆï¼Œæ˜¯é€šè¿‡ä¸åŒçš„å¯å­¦ä¹ å‚æ•°çš„æ˜ å°„å±‚å®ç°ä¸åŒmodalityçš„è§‚æµ‹tokençš„èåˆå¤„ç†çš„å—ï¼Ÿ

å›ç­”ï¼šget_model_configå‡½æ•°é‡Œæœ‰å®šä¹‰ï¼Œé’ˆå¯¹ä¸åŒçš„è§‚æµ‹ç±»å‹æœ‰ä¸åŒçš„tokenizerï¼Œå¦‚å›¾åƒï¼ˆå’Œè‡ªç„¶æŒ‡ä»¤èåˆï¼‰çš„tokenizerä¸ºImageTokenizerï¼ŒåŒè‡‚æœºå™¨äººï¼Œå››ç»„æœºå™¨äººç­‰çš„æœ¬ä½“æ„ŸçŸ¥è§‚æµ‹æ•°æ®çš„tokenizerä¸ºLowdimObsTokenizerã€‚


```
# https://github.com/rail-berkeley/crossformer/blob/4a56b64411e7ec039ea6cce6bbbe65a38f677db1/scripts/configs/pretrain_config.py#L221

def get_model_config(transformer_size):
    token_embedding_size, transformer_kwargs = common_transformer_sizes(
        transformer_size
    )

    encoder = ModuleSpec.create(ResNet26FILM)
    return dict(
        observation_tokenizers=dict(
            primary=ModuleSpec.create(
                ImageTokenizer,
                obs_stack_keys=["image_primary"],
                task_stack_keys=["image_primary"],
                task_film_keys=["language_instruction"],
                encoder=encoder,
            ),
            high=ModuleSpec.create(
                ImageTokenizer,
                obs_stack_keys=["image_high"],
                task_stack_keys=["image_high"],
                task_film_keys=["language_instruction"],
                encoder=encoder,
            ),
            nav=ModuleSpec.create(
                ImageTokenizer,
                obs_stack_keys=["image_nav"],
                task_stack_keys=["image_nav"],
                task_film_keys=[],
                encoder=ModuleSpec.create(ResNet26),
            ),
            left=ModuleSpec.create(
                ImageTokenizer,
                obs_stack_keys=["image_left_wrist"],
                task_stack_keys=[],
                task_film_keys=["language_instruction"],
                encoder=encoder,
            ),
            right=ModuleSpec.create(
                ImageTokenizer,
                obs_stack_keys=["image_right_wrist"],
                task_stack_keys=[],
                task_film_keys=["language_instruction"],
                encoder=encoder,
            ),
            bimanual=ModuleSpec.create(
                LowdimObsTokenizer,
                obs_keys=["proprio_bimanual"],
                dropout_rate=0.2,
            ),
            quadruped=ModuleSpec.create(
                LowdimObsTokenizer,
                obs_keys=["proprio_quadruped"],
            ),
        ),
        task_tokenizers=dict(),
        heads=dict(
            bimanual=ModuleSpec.create(
                L1ActionHead,
                action_horizon=100,
                action_dim=BIMANUAL_ACTION_DIM,
                num_preds=BIMANUAL_ACTION_DIM,
                pool_strategy="pass",
                readout_key="readout_bimanual",
                clip_pred=False,
                loss_weight=1.0,
                constrain_loss_dims=True,
            ),
            single_arm=ModuleSpec.create(
                L1ActionHead,
                action_horizon=4,
                action_dim=SINGLE_ARM_ACTION_DIM,
                num_preds=SINGLE_ARM_ACTION_DIM,
                pool_strategy="pass",
                readout_key="readout_single_arm",
                clip_pred=False,
                loss_weight=1.0,
                constrain_loss_dims=True,
            ),
            nav=ModuleSpec.create(
                L1ActionHead,
                action_horizon=4,
                action_dim=NAV_ACTION_DIM,
                num_preds=NAV_ACTION_DIM,
                pool_strategy="pass",
                readout_key="readout_nav",
                clip_pred=False,
                loss_weight=1.0,
                constrain_loss_dims=True,
            ),
            quadruped=ModuleSpec.create(
                L1ActionHead,
                action_horizon=1,
                action_dim=QUADRUPED_ACTION_DIM,
                num_preds=QUADRUPED_ACTION_DIM,
                pool_strategy="pass",
                readout_key="readout_quadruped",
                clip_pred=False,
                loss_weight=1.0,
                constrain_loss_dims=True,
            ),
        ),
        readouts=dict(bimanual=100, single_arm=4, nav=4, quadruped=1),
        token_embedding_size=token_embedding_size,
        transformer_kwargs=transformer_kwargs,
        max_horizon=10,
    )

# é’ˆå¯¹ä¸åŒç±»å‹çš„observationsï¼Œé€šè¿‡projectionæ˜ å°„åˆ°embedding_sizeå¤§å°
# https://github.com/rail-berkeley/crossformer/blob/4a56b64411e7ec039ea6cce6bbbe65a38f677db1/crossformer/model/crossformer_module.py#L197
for name, tok in self.observation_tokenizers.items():
    group_name = f"obs_{name}"
    # Receive inputs from tokenizer and cast to embedding size
    tokenizer_output: TokenGroup = tok(observations, tasks, train=train)
    if tokenizer_output is None:
        logging.warning(f"Skipping observation tokenizer: {group_name}")
        continue

    obs_tokens = nn.Dense(
        self.token_embedding_size, name=f"{group_name}_projection"
    )(tokenizer_output.tokens)
    # obs_tokens shape is (batch, horizon, n_tokens, token_embedding_size)

    # create positional embedding
    obs_pos_enc = self._create_positional_embedding(group_name, obs_tokens)

    # Update mask to account for which timesteps are padding
    obs_pad_mask = jnp.logical_and(
        timestep_pad_mask[:, :, None], tokenizer_output.mask
    )

    all_timestep_groups.append(
        TimestepGroup(
            tokens=obs_tokens,
            pos_enc=obs_pos_enc,
            mask=obs_pad_mask,
            name=group_name,
            attention_rules=observation_attention_rules,
        )
    )
```


## é—®é¢˜3ï¼šreadoutsçš„tokenå…·ä½“æœ‰å“ªäº›ï¼Œå¦‚ä½•å®šä¹‰çš„ï¼Œå’ŒBERTæ¨¡å‹çš„[CLS]ç±»ä¼¼å—ï¼Ÿ

å›ç­”ï¼šåœ¨è¯¥å¼€æºç³»ç»Ÿçš„å®ç°ä¸­ï¼Œreadoutsä¸å¯¹åº”ä»»åŠ¡è¾“å…¥ï¼Œåœ¨å®ç°ä¸Šä»¥ç”Ÿæˆä½ç½®åµŒå…¥çš„å½¢å¼å­˜åœ¨ï¼Œä½†å’Œå…¶ä»–çš„tokensä¸æ˜¯è¿›è¡Œelementwiseçš„æ“ä½œï¼Œè€Œæ˜¯concatenateåˆ°observationsçš„tokensåé¢ã€‚

```
# https://github.com/rail-berkeley/crossformer/blob/4a56b64411e7ec039ea6cce6bbbe65a38f677db1/crossformer/model/crossformer_module.py#L249
for readout_name in readouts:
    group_name = f"readout_{readout_name}"
    # Readouts do not correspond to any inputs, just positional embeddings
    n_tokens_for_readout = self.readouts[readout_name]
    readout_tokens = jnp.zeros(
        (batch_size, horizon, n_tokens_for_readout, self.token_embedding_size)
    )

    # create positional embedding
    readout_pos_enc = self._create_positional_embedding(
        group_name, readout_tokens
    )
    readout_mask = jnp.ones((batch_size, horizon, n_tokens_for_readout))
    readout_attention_rules = {
        "task_*": AttentionRule.CAUSAL,
        "obs_*": AttentionRule.CAUSAL,
        group_name: AttentionRule.CAUSAL,
    }  # Attend to tasks, all previous observations, and *only it's own own readout*

    all_timestep_groups.append(
        TimestepGroup(
            tokens=readout_tokens,
            pos_enc=readout_pos_enc,
            mask=readout_mask,
            name=group_name,
            attention_rules=readout_attention_rules,
        )
    )

# https://github.com/rail-berkeley/crossformer/blob/4a56b64411e7ec039ea6cce6bbbe65a38f677db1/crossformer/model/crossformer_module.py#L315
def _create_positional_embedding(self, name: str, tokens: jax.Array):
    if tokens.ndim == 3:  # for prefixes
        shape = (1, *tokens.shape[-2:])
    elif (
        tokens.ndim == 4
    ):  # for timesteps, create embedding for max_horizon, then truncate
        shape = (1, self.max_horizon, *tokens.shape[-2:])
    else:
        raise ValueError(f"Invalid tokens shape: {tokens.shape}")

    embedding = self.param(
        f"{name}_pos_embedding",
        nn.initializers.normal(stddev=0.02),
        shape,
    )
    if tokens.ndim == 4:
        # Use only the timesteps we receive as input
        embedding = embedding[:, : tokens.shape[1]]
    return jnp.broadcast_to(embedding, tokens.shape)

```

## é—®é¢˜4ï¼š å¼‚æ„çš„æœºå™¨äººæ•°æ®é›†å¦‚ä½•åœ¨ä¸€èµ·è®­ç»ƒï¼Œä¸åŒhorizoneçª—å£çš„æ•°æ®å¯¹åº”çš„tokensè¾“å…¥åºåˆ—çš„é•¿åº¦å¤§å°ä¸åŒï¼Œæ˜¯é€šè¿‡paddingå’Œmaskç›¸ç»“åˆè¿›è¡Œç»Ÿä¸€å¤„ç†çš„å—ï¼Ÿä»¥åŠæœ‰ä»€ä¹ˆæ ‡è®°æ¥å°†transformerçš„outputçš„embeddingsè¾“å‡ºç»™å¯¹åº”çš„ä»»åŠ¡headå—ï¼Ÿ

å›ç­”ï¼šé¦–å…ˆé’ˆå¯¹ä¸åŒçš„æ•°æ®é›†çš„ç‰¹å®šæ ¼å¼å®šä¹‰äº†è½¬æ¢å‡½æ•°å°†æ•°æ®è½¬æ¢æˆç»Ÿä¸€çš„å½¢å¼ï¼šOXE_STANDARDIZATION_TRANSFORMSã€‚
å’Œä¸€èˆ¬çš„LLMæ¨¡å‹çš„æ‰¹å¤„ç†æ–¹æ³•ç±»ä¼¼ï¼Œç¡®å®æ˜¯é€šè¿‡maskå®ç°äº†ä¸åŒé•¿åº¦åºåˆ—çš„æ•°æ®çš„ç»Ÿä¸€è®­ç»ƒï¼Œå¯ä»¥åœ¨`generate_attention_mask`å‡½æ•°ä¸­çœ‹ä¸€ä¸‹å…·ä½“çš„å®ç°


```
# https://github.com/rail-berkeley/crossformer/blob/4a56b64411e7ec039ea6cce6bbbe65a38f677db1/crossformer/model/crossformer_module.py#L249
# å®šä¹‰äº†readoutçš„groupå…·ä½“çš„name

for readout_name in readouts:
    group_name = f"readout_{readout_name}"
    # Readouts do not correspond to any inputs, just positional embeddings
    n_tokens_for_readout = self.readouts[readout_name]
    readout_tokens = jnp.zeros(
        (batch_size, horizon, n_tokens_for_readout, self.token_embedding_size)
    )

    # create positional embedding
    readout_pos_enc = self._create_positional_embedding(
        group_name, readout_tokens
    )
    readout_mask = jnp.ones((batch_size, horizon, n_tokens_for_readout))
    readout_attention_rules = {
        "task_*": AttentionRule.CAUSAL,
        "obs_*": AttentionRule.CAUSAL,
        group_name: AttentionRule.CAUSAL,
    }  # Attend to tasks, all previous observations, and *only it's own own readout*

    all_timestep_groups.append(
        TimestepGroup(
            tokens=readout_tokens,
            pos_enc=readout_pos_enc,
            mask=readout_mask,
            name=group_name,
            attention_rules=readout_attention_rules,
        )
    )

# å¹¶åœ¨æ­¤ä½ç½®å¯¹è¾“å‡ºæŒ‰groun_nameç”Ÿæˆoutputså­—å…¸
# æ¨¡å‹è¾“å‡ºçš„å•ä¸€åºåˆ—æ‹†è§£å›åŸå§‹ç»„ç»“æ„
https://github.com/rail-berkeley/crossformer/blob/4a56b64411e7ec039ea6cce6bbbe65a38f677db1/crossformer/model/crossformer_module.py#L292
outputs.update(
    {
        group.name: TokenGroup(group.tokens, group.mask)
        for group in timestep_outputs
    }
)

# è¿›ä¸€æ­¥è¾“é€ç»™å¤šä¸ªhead
# https://github.com/rail-berkeley/crossformer/blob/4a56b64411e7ec039ea6cce6bbbe65a38f677db1/crossformer/model/crossformer_module.py#L366

head_outputs = {}
for head_name, head in self.heads.items():
    head_outputs[head_name] = head(transformer_outputs, train=train)
return transformer_outputs, head_outputs


# æ¯ä¸€ä¸ªheadæ ¹æ®å¯¹åº”çš„tokensè¾“å‡ºè¿›è¡Œæ¨ç†
# https://github.com/rail-berkeley/crossformer/blob/4a56b64411e7ec039ea6cce6bbbe65a38f677db1/crossformer/model/components/action_heads.py#L125

token_group = transformer_outputs[self.readout_key]
assert token_group.tokens.ndim == 4, (
    f"Expected token_group.tokens to have shape (batch_size, window_size, num_tokens, embedding_size), "
    f"but got shape {token_group.tokens.shape}"
)
if self.pool_strategy == "use_map":  # Multi-head attention pooling
    embeddings = self.map_head(token_group, train=train)[:, :, 0]
elif self.pool_strategy == "mean":  # mean pooling
    embeddings = token_group.tokens.mean(axis=-2)
elif self.pool_strategy == "pass":
    embeddings = token_group.tokens
else:
    raise ValueError(f"{self.pool_strategy} not implemented!")

if len(embeddings.shape) == 3:
    # Implies embeddings is (batch_size, window_size, embedding_size)
    mean = self.mean_proj(embeddings)
    mean = rearrange(
        mean, "b w (h a) -> b w h a", h=self.action_horizon, a=self.action_dim
    )
else:
    # Assumes embeddings is (batch_size, window_size, H, embedding_size)
    assert embeddings.shape[-2] == self.action_horizon
    mean = self.mean_proj(embeddings)

if self.clip_pred:
    mean = jnp.tanh(mean / self.max_action) * self.max_action

return mean


# è®­ç»ƒæ—¶è¿˜éœ€è¦è®¡ç®—loss
https://github.com/rail-berkeley/crossformer/blob/4a56b64411e7ec039ea6cce6bbbe65a38f677db1/crossformer/model/components/action_heads.py#L161

if self.constrain_loss_dims:
    # when using separate heads we can constrain the loss to the action dimensions and action horizon specific to this head
    actions = actions[:, :, : self.action_horizon, : self.action_dim]
    action_pad_mask = action_pad_mask[
        :, :, : self.action_horizon, : self.action_dim
    ]

# (batch, window_size, action_horizon, action_dim)
mean = self(transformer_outputs, train=train)

if action_head_mask is None:
    action_head_mask = jnp.ones(mean.shape[0], dtype=bool)

# combine the timestep pad mask with the action pad mask and the action head mask
mask = (
    timestep_pad_mask[:, :, None, None]
    & action_pad_mask
    & action_head_mask[:, None, None, None]
)

loss, metrics = continuous_loss(mean, actions, mask, loss_type=self.loss_type)
return loss, metrics
```


## é—®é¢˜5ï¼š å¼‚æ„çš„æœºå™¨äººæ•°æ®é›†å¤„ç†æ–¹é¢æœ‰å“ªäº›ç‰¹æ®Šçš„éœ€è¦æ³¨æ„çš„åœ°æ–¹ï¼Ÿ

å›ç­”ï¼šå›ç­”ï¼šé¦–å…ˆé’ˆå¯¹ä¸åŒçš„æ•°æ®é›†çš„ç‰¹å®šæ ¼å¼å®šä¹‰äº†è½¬æ¢å‡½æ•°å°†æ•°æ®è½¬æ¢æˆç»Ÿä¸€çš„å½¢å¼ï¼šOXE_STANDARDIZATION_TRANSFORMS


```
# https://github.com/rail-berkeley/crossformer/blob/4a56b64411e7ec039ea6cce6bbbe65a38f677db1/crossformer/data/oxe/oxe_standardization_transforms.py#L1070

OXE_STANDARDIZATION_TRANSFORMS = {
    "bridge_dataset": bridge_dataset_transform,
    "fractal20220817_data": rt1_dataset_transform,
    "kuka": kuka_dataset_transform,
    "taco_play": taco_dataset_transform,
    "taco_extra": taco_dataset_transform,
    "jaco_play": jaco_play_dataset_transform,
    "berkeley_cable_routing": berkeley_cable_routing_dataset_transform,
    "roboturk": roboturk_dataset_transform,
    "nyu_door_opening_surprising_effectiveness": nyu_door_opening_dataset_transform,
    "viola": viola_dataset_transform,
    "berkeley_autolab_ur5": berkeley_autolab_ur5_dataset_transform,
    "toto": toto_dataset_transform,
    "language_table": language_table_dataset_transform,
    "columbia_cairlab_pusht_real": pusht_dataset_transform,
    "stanford_kuka_multimodal_dataset_converted_externally_to_rlds": stanford_kuka_multimodal_dataset_transform,
    "nyu_rot_dataset_converted_externally_to_rlds": nyu_rot_dataset_transform,
    "stanford_hydra_dataset_converted_externally_to_rlds": stanford_hydra_dataset_transform,
    "austin_buds_dataset_converted_externally_to_rlds": austin_buds_dataset_transform,
    "nyu_franka_play_dataset_converted_externally_to_rlds": nyu_franka_play_dataset_transform,
    "maniskill_dataset_converted_externally_to_rlds": maniskill_dataset_transform,
    "furniture_bench_dataset_converted_externally_to_rlds": furniture_bench_dataset_transform,
    "cmu_franka_exploration_dataset_converted_externally_to_rlds": cmu_franka_exploration_dataset_transform,
    "ucsd_kitchen_dataset_converted_externally_to_rlds": ucsd_kitchen_dataset_transform,
    "ucsd_pick_and_place_dataset_converted_externally_to_rlds": ucsd_pick_place_dataset_transform,
    "austin_sailor_dataset_converted_externally_to_rlds": austin_sailor_dataset_transform,
    "austin_sirius_dataset_converted_externally_to_rlds": austin_sirius_dataset_transform,
    "bc_z": bc_z_dataset_transform,
    "utokyo_pr2_opening_fridge_converted_externally_to_rlds": tokyo_pr2_opening_fridge_dataset_transform,
    "utokyo_pr2_tabletop_manipulation_converted_externally_to_rlds": tokyo_pr2_tabletop_manipulation_dataset_transform,
    "utokyo_xarm_pick_and_place_converted_externally_to_rlds": utokyo_xarm_pick_place_dataset_transform,
    "utokyo_xarm_bimanual_converted_externally_to_rlds": utokyo_xarm_bimanual_dataset_transform,
    "robo_net": robo_net_dataset_transform,
    "berkeley_mvp_converted_externally_to_rlds": berkeley_mvp_dataset_transform,
    "berkeley_rpt_converted_externally_to_rlds": berkeley_rpt_dataset_transform,
    "kaist_nonprehensile_converted_externally_to_rlds": kaist_nonprehensible_dataset_transform,
    "stanford_mask_vit_converted_externally_to_rlds": stanford_mask_vit_dataset_transform,
    "tokyo_u_lsmo_converted_externally_to_rlds": tokyo_lsmo_dataset_transform,
    "dlr_sara_pour_converted_externally_to_rlds": dlr_sara_pour_dataset_transform,
    "dlr_sara_grid_clamp_converted_externally_to_rlds": dlr_sara_grid_clamp_dataset_transform,
    "dlr_edan_shared_control_converted_externally_to_rlds": dlr_edan_shared_control_dataset_transform,
    "asu_table_top_converted_externally_to_rlds": asu_table_top_dataset_transform,
    "stanford_robocook_converted_externally_to_rlds": robocook_dataset_transform,
    "imperialcollege_sawyer_wrist_cam": imperial_wristcam_dataset_transform,
    "iamlab_cmu_pickup_insert_converted_externally_to_rlds": iamlab_pick_insert_dataset_transform,
    "uiuc_d3field": uiuc_d3field_dataset_transform,
    "utaustin_mutex": utaustin_mutex_dataset_transform,
    "berkeley_fanuc_manipulation": berkeley_fanuc_dataset_transform,
    "cmu_playing_with_food": cmu_playing_with_food_dataset_transform,
    "cmu_play_fusion": playfusion_dataset_transform,
    "cmu_stretch": cmu_stretch_dataset_transform,
    "omnimimic_gnm_dataset": omnimimic_gnm_transform,
    "aloha_dagger_dataset": aloha_dataset_transform,
    "aloha_mobile_dataset": aloha_dataset_transform,
    "fmb_dataset": fmb_dataset_transform,
    "dobbe": dobbe_dataset_transform,
    "roboset": roboset_dataset_transform,
    "rh20t": rh20t_dataset_transform,
    "mujoco_manip": mujoco_manip_dataset_transform,
    "go1": go1_dataset_transform,
    "go1_real_dataset": go1_real_dataset_transform,
    "a1": go1_dataset_transform,
    "aloha_pen_uncap_diverse_dataset": aloha_pen_uncap_dataset_transform,
    "aloha_new_sushi_dataset": aloha_pen_uncap_dataset_transform,
    "aloha_dough_cut_dataset": aloha_dough_dataset_transform,
    "aloha_lucy_dataset": aloha_dough_dataset_transform,
    "aloha_drawer_dataset": aloha_dough_dataset_transform,
    "aloha_pick_place_dataset": aloha_dough_dataset_transform,
    "aloha_static_dataset": aloha_dough_dataset_transform,
    "aloha_sushi_cut_full_dataset": aloha_dough_dataset_transform,
    "droid": droid_dataset_transform,
    "droid_wipe": droid_dataset_transform,
    "droid_flip_pot_upright": droid_dataset_transform,
}
```

## é—®é¢˜ï¼šå•è‡‚æœºå™¨äººçš„ä»»åŠ¡ä¸€èˆ¬æ˜¯æŠ“å–å’Œæ”¾ç½®ï¼Œå› æ­¤ä¸€èˆ¬çš„actionåªéœ€è¦å¼€å§‹å’Œä»»åŠ¡å®Œæˆæ—¶çš„åŠ¨ä½œï¼Œå…¶ä»–çš„å¯ä»¥äº¤ç»™è§„åˆ’æ¥å®Œæˆï¼Ÿï¼Ÿ

from chatgpt
ä½ è¯´å¾—éå¸¸å¯¹ï¼å¯¹äºå•è‡‚æœºå™¨äººæ¥è¯´ï¼Œ**ä»»åŠ¡ï¼ˆå¦‚æŠ“å–å’Œæ”¾ç½®ï¼‰æœ¬è´¨ä¸Šå¯ä»¥è¢«ç®€åŒ–ä¸ºå‡ ä¸ªå…³é”®å¸§æˆ–å…³é”®åŠ¨ä½œ**ï¼Œå…¶ä½™çš„åŠ¨ä½œå¯ä»¥ç”±åº•å±‚æ§åˆ¶æˆ–è§„åˆ’å™¨æ¥å¹³æ»‘è¡¥å…¨ã€‚è¿™ç§åšæ³•åœ¨è®¸å¤šå·¥ä¸šå’Œç ”ç©¶ç³»ç»Ÿä¸­éƒ½éå¸¸å¸¸è§ã€‚

---

## âœ… ä¸ºä»€ä¹ˆå•è‡‚æœºå™¨äººä»»åŠ¡é€šå¸¸åªéœ€è¦å°‘é‡åŠ¨ä½œï¼ˆä¾‹å¦‚å¼€å§‹å’Œç»“æŸï¼‰ï¼Ÿ

### 1. **ä»»åŠ¡ç»“æ„ç®€å•**
- æŠ“å–-æ”¾ç½®ä»»åŠ¡å¾€å¾€åˆ†ä¸ºï¼š
  - **æ¥è¿‘ç›®æ ‡ï¼ˆapproachï¼‰**
  - **é—­åˆå¤¹çˆªï¼ˆgraspï¼‰**
  - **ç§»åŠ¨åˆ°ç›®æ ‡ä½ç½®**
  - **é‡Šæ”¾ï¼ˆreleaseï¼‰**
- ä¸­é—´çš„è¿‡ç¨‹ï¼Œæ¯”å¦‚è·¯å¾„è§„åˆ’ã€é¿éšœç­‰ï¼Œå®Œå…¨å¯ä»¥é€šè¿‡**è¿åŠ¨è§„åˆ’å™¨**ï¼ˆå¦‚ MoveItã€RRTã€CHOMPï¼‰æˆ–**è½¨è¿¹ç”Ÿæˆå™¨**è‡ªåŠ¨å®Œæˆã€‚

---

### 2. **å…³é”®å¸§è§„åˆ’æ€è·¯**
ä½ å¯ä»¥æŠŠå•è‡‚åŠ¨ä½œçœ‹æˆæ˜¯ï¼š
```python
å…³é”®åŠ¨ä½œåºåˆ— = [
  {"pose": grasp_pose, "action": "close_gripper"},
  {"pose": place_pose, "action": "open_gripper"}
]
```

æ¨¡å‹åªéœ€é¢„æµ‹è¿™ä¸¤ä¸ª poseï¼Œè§„åˆ’å™¨è´Ÿè´£åœ¨è¿™ä¸¤ä¸ªå…³é”®å¸§ä¹‹é—´æ’å€¼å‡ºå¹³æ»‘çš„è½¨è¿¹ã€‚

---

### 3. **é¢„æµ‹ horizon å°å¯ä»¥å‡å°‘è´Ÿæ‹…**
- åœ¨ imitation learning æˆ– transformer-based policy ä¸­ï¼Œè®¾ç½® `action_horizon=4` è¶³ä»¥è¦†ç›–â€œå…³é”®å¸§+å°‘é‡è¿‡æ¸¡åŠ¨ä½œâ€ã€‚
- å¤ªé•¿çš„ action horizon å¯¹å•è‡‚ä»»åŠ¡åè€Œä¼šå¼•å…¥**ä¸å¿…è¦çš„å¤æ‚æ€§**ï¼Œä¸”é¢„æµ‹ä¸ç¨³å®šã€‚

---

## ğŸš§ ä¸¾ä¸ªåä¾‹ï¼šä¸ºä»€ä¹ˆ bimanual ä¸è¡Œï¼Ÿ

- åŒè‡‚ä»»åŠ¡ä¸­ï¼Œä¸¤ä¸ªæ‰‹è‡‚è¦**å¼ºè€¦åˆååŒ**ï¼Œä¾‹å¦‚å·¦æ‰‹æ‰¶ç¨³ã€å³æ‰‹æ‹§ç“¶ç›–ã€‚
- ä¸­é—´è¿‡ç¨‹æ— æ³•ç®€å•æ’å€¼ï¼Œå¿…é¡»åŒæ—¶è€ƒè™‘ä¸¤ä¸ªæ‰‹çš„æ—¶åºå’Œç©ºé—´é…åˆï¼Œä¸èƒ½ç®€å•äº¤ç»™ plannerã€‚
- æ‰€ä»¥éœ€è¦é•¿çš„ `action_horizon` æ¥æ•æ‰å¤æ‚ååŒç­–ç•¥ã€‚

---

## âœ… æ€»ç»“

| ä»»åŠ¡ç±»å‹       | æ§åˆ¶ç­–ç•¥                    | æ¨¡å‹é¢„æµ‹éœ€æ±‚       |
|----------------|-----------------------------|--------------------|
| å•è‡‚æŠ“æ”¾       | å…³é”®å¸§ + è¿åŠ¨è§„åˆ’           | é¢„æµ‹å°‘é‡åŠ¨ä½œ       |
| åŒè‡‚ååŒ       | æ•´ä½“è½¨è¿¹è§„åˆ’ï¼ˆæ—¶åºè€¦åˆï¼‰     | é¢„æµ‹é•¿åºåˆ—         |
| å››è¶³æ­¥æ€æ§åˆ¶   | å®æ—¶åé¦ˆæ§åˆ¶                 | æ¯æ­¥å®æ—¶é¢„æµ‹       |
